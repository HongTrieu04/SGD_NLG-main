---
# Meta Info
  ModelName: gpt2-medium-SGD
  ExpName: &exp_name gpt2-mediums-naive
  VerName: &ver_name Adam-b5
  WandbProject: &wandb_project bbyrne-nlg
  Note: "gpt2-medium for SGD, using naive linearization"

# Tokenizer
  TokenizerInfo:
    tokenizer_class: GPT2TokenizerFast
    tokenizer_name: &tk_name gpt2
    tokenizer_alias: &tk_alias GPT2TokenizerFast

# LightningModule
  LightningModuleName: HFGenerationModel
  LightningModuleParas:
    model_class: GPT2LMHeadModel
    model_path: gpt2-medium
    optimizer: AdamW
    optimizer_params:
      lr: 0.00005
    
# LightningDataModule
  LightningDataModuleName: "GEMSGD_DataModule"
  LightningDataModuleParas:
    batch_size: 1
    tokenizer_name: *tk_alias
    force_process: false
    save_cache: true
    encode_args:
      padding: max_length
      truncation: true
    linearizer_class: SGD_PaperNaiveLinearizer
    schema_paths:
      - data/schemas/schema-train.json
      - data/schemas/schema-test.json
      - data/schemas/schema-dev.json
    template_dir: data/utterance_templates

# Training
  TrainerParas:
    # accumulate_grad_batches: 32
    max_epochs: 5
    val_check_interval: 10000
  
  ModelCheckpointParas:
    monitor: 'val_loss'

#Logging
  TrainLoggerName: WandbLogger
  TrainLoggerParas:
    project: *wandb_project
    name: *exp_name
    version: *ver_name

  # TrainLoggerName: TensorBoardLogger
  # TrainLoggerParas:
  #   name: *exp_name
  #   version: *ver_name

  TestLoggerName: CSVLogger
  TestLoggerParas:
    name: *exp_name
    version: test-gpt2-medium-naive

# Testing
  LoadingParas:
    checkpoint_path: "bbyrne-nlg/AdamW-b8/checkpoints/epoch=2-step=61245.ckpt"
    save_decode: True
    decode_path: logs/gpt2-schema-guided/test-AdamW-b8
    generate_params:
      num_beams: 4
      length_penalty: 0.6
      max_length: 1500
      early_stopping: true
